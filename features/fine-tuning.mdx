---
title: 'ðŸª„ Zero-Code Fine Tuning'
description: 'Oxen.ai lets you fine-tune models on your own data without writing any code.'
---

Fine-tuning unlocks powerful customization capabilities for large language models when basic prompting falls short or when you're facing throughput and latency constraints. With Oxen.ai, we make it easy to fine-tune LLMs on your own data without writing any code. 

Simply upload your data, and we will provision optimized GPU infrastructure to execute the training process, then save the fine-tuned model weights directly to your repository.

![Fine-Tuning Ox](/images/fine_tuning/fine-tune-ox.jpg)

## Uploading a Dataset

To get started, you'll need to create a new repository on Oxen.ai. Once you've created a repository, you can upload your data. The dataset can be in any tabular format including CSV, JSON, Parquet, or Arrow.

![Fine-Tuning Dataset Upload](/images/fine_tuning/fine-tune-upload-file.png)

Once you have your dataset uploaded, you can query, explore, and make sure that the data is high quality before kicking off the fine-tuning process. Your model will only be as good as the data you train it on.

![Fine-Tuning Dataset](/images/fine_tuning/fine-tune-dataset.png)

## Selecting a Model

When you feel confident that your dataset is ready, use the "Actions" button to select the model you want to fine-tune.

![Fine-Tuning Dataset](/images/fine_tuning/fine-tune-actions.png)

This will take you to a form where you can select the model you want to fine-tune and the columns you want to use for the fine-tuning process. Right now we support fine-tuning for prompt/response single-turn chat pairs.

![Fine-Tuning Model Selection](/images/fine_tuning/fine-tune-model-selection.png)

[Contact us](https://airtable.com/appDW4XBL7qTihmwi/shrQF72gHTJw8zvie) if you need to fine-tune a different model or have more complex data formats or use cases.

## Monitoring the Fine-Tune

Once you have started the fine-tuning process, you can monitor its progress. The dashboard will show you loss over time, token accuracy, the learning rate, and number of tokens processed.

![Fine-Tuning Monitoring](/images/fine_tuning/fine-tune-monitoring.png)

Click on the "Configuration" tab to see the fine-tuning configuration. This will include a link to the dataset version you used and the raw model weights. It will show you the pricing for the fine-tuning process as well.

If you want access to the raw model weights, you can download them from the repository using the Oxen.ai Python library or the CLI.

<CodeGroup>

```python Python
from oxen import RemoteRepo

repo = RemoteRepo("my-username/my-repo")

repo.download("models", revision="Qwen3-0.6B-experiment-1_fte7562a9e")
```

```bash CLI
oxen download my-username/my-repo models --revision Qwen3-0.6B-experiment-1_fte7562a9e
```

</CodeGroup>

![Fine-Tuning Configuration](/images/fine_tuning/fine-tune-model-weights.png)


## Deploying the Model

Once the model is fine-tuned, you can deploy it to a hosted endpoint. This will give you a `/chat/completions` endpoint that you can use to test out the model.

![Fine-Tuning Configuration](/images/fine_tuning/fine-tune-deploy-model.png)

Swap out the model name with the name of the model you want to use.

```bash
curl https://hub.oxen.ai/api/chat/completions -H "Content-Type: application/json" -d '{
    "model":"oxenai:my-model-name",
    "messages": [{"role": "user", "content": "What is the best name for a friendly ox?"}],
}'
```

If you need custom or private deployments in your own VPC, [contact us](https://airtable.com/appDW4XBL7qTihmwi/shrQF72gHTJw8zvie) to learn more.

## Chatting with the Model

Once the model is deployed, you can also chat with it using the Oxen.ai chat interface. Learn more about the [chat interface here](/features/chat).
