---
title: 'Available Fine-Tuning Models'
description: 'Models currently supported for fine-tuning on Oxen.ai.'
---

## Overview

Oxen.ai supports fine-tuning on a curated set of models across different modalities.

This page lists the models that are **currently fine-tuneable**, along with the fine-tuning task type and expected dataset schema.

> Other models listed in the model catalog (for example GPT 5, o3, Llama 3.1, Gemini, Claude, general Qwen2.5 models, etc.) are **not** yet available for fine-tuning on Oxen and can only be used for inference.

## Image Editing Models

### Qwen/Qwen-Image-Edit

- **Slug**: `qwen-image-edit`
- **Provider**: Replicate (`qwen/qwen-image-edit`)
- **Modality**: Image → Image
- **Pricing**: `$0.03` / image
- **Fine-tune task**: `image_editing`

**Expected dataset schema**

Your dataset should contain at least the following columns:

- **`control_image_column`** – Input or reference image to edit (for example: the original product or character image).
- **`caption_column`** – Text prompt or instruction describing the desired edit.
- **`image_column`** – Target/output image showing the desired edited result.

These field names are provided when you configure `training_params` for the fine-tune.

### Qwen/Qwen-Image-Edit-Plus

- **Slug**: `qwen-image-edit-plus`
- **Provider**: Replicate (`qwen/qwen-image-edit-plus`)
- **Modality**: Multi-Image → Image
- **Pricing**: `$0.03` / image
- **Fine-tune task**: `multi_image_editing`

**Expected dataset schema**

Your dataset should contain at least the following columns:

- **`control_image_columns`** – Array of one or more input image columns (for example: `person_image`, `product_image`, `background_image`).
- **`caption_column`** – Text prompt or instruction describing the composition or edit.
- **`image_column`** – Target/output image.

Use this model when you want to condition on **multiple input images** at once (for example, combining a person and a product into a new scene).

## Text-to-Video Models

### Wan-AI/Wan2.1-T2V-1.3B-Diffusers

- **Slug**: `wan-ai-wan2-1-t2v-1-3b-diffusers`
- **Provider**: Baseten (`Wan-AI/Wan2.1-T2V-1.3B-Diffusers`)
- **Modality**: Text → Video
- **Pricing**: `$0.002` / second
- **Fine-tune task**: `text_to_video`

**Expected dataset schema**

Your dataset should contain at least the following columns:

- **`caption_column`** – Text prompt describing the video content.
- **`image_column`** – Video data or frames reference (for example: a path to a video file or a representative frame image, depending on your data preparation).

The exact data format for `image_column` depends on how your repository stores video data (for example, paths in Parquet vs. pre-extracted frames).

### Wan-AI/Wan2.2-T2V-A14B-Diffusers

- **Slug**: `wan-ai-wan2-2-t2v-a14b-diffusers`
- **Provider**: Baseten (`Wan-AI/Wan2.2-T2V-A14B-Diffusers`)
- **Modality**: Text / Image → Video
- **Pricing**: time-based (see model card for current rate)
- **Fine-tune task**: `text_to_video`

**Expected dataset schema**

Your dataset should contain at least the following columns:

- **`caption_column`** – Text prompt describing the video.
- **`image_column`** – Corresponding video data or frame reference.

This model is higher capacity and is designed for **higher-fidelity** text-to-video and image-to-video generation (for example: 720p, 24fps outputs).

## Summary of Fine-Tuneable Models

| Display Name                         | Slug                                   | Modality              | Fine-Tune Task        |
|--------------------------------------|----------------------------------------|-----------------------|-----------------------|
| Qwen/Qwen-Image-Edit                 | `qwen-image-edit`                      | Image → Image         | `image_editing`       |
| Qwen/Qwen-Image-Edit-Plus            | `qwen-image-edit-plus`                 | Multi-Image → Image   | `multi_image_editing` |
| Wan-AI/Wan2.1-T2V-1.3B-Diffusers     | `wan-ai-wan2-1-t2v-1-3b-diffusers`     | Text → Video          | `text_to_video`       |
| Wan-AI/Wan2.2-T2V-A14B-Diffusers     | `wan-ai-wan2-2-t2v-a14b-diffusers`     | Text / Image → Video  | `text_to_video`       |

You can pass any of the slugs above as the `base_model` when creating a fine-tune via the HTTP API, as long as the `script_type` and `training_params` match the expected task and dataset schema.
